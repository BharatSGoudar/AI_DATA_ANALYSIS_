{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Introduction to Isolation Forest\n",
    "**Description**: Install the necessary library and load a sample dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Install scikit-learn\n",
    "2. Load a sample dataset using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "# Step 1: Install scikit-learn\n",
    "# If running in a notebook or script, uncomment the line below\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Step 2: Load a sample dataset\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the iris dataset as an example\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Building an Isolation Forest\n",
    "**Description**: Initialize an Isolation Forest model and fit it to the Boston dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Initialize Isolation Forest\n",
    "2. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load Boston dataset\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "df = boston.frame.dropna()\n",
    "\n",
    "# Optional: Scale features for better model performance\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop('MEDV', axis=1))\n",
    "\n",
    "# Step 2: Initialize and fit Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_forest.fit(scaled_data)\n",
    "\n",
    "# Predict anomalies\n",
    "df['anomaly'] = iso_forest.predict(scaled_data)\n",
    "df['anomaly'] = df['anomaly'].map({1: 0, -1: 1})  # 1 for anomaly\n",
    "\n",
    "# View results\n",
    "print(df[['MEDV', 'anomaly']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Detecting Anomalies\n",
    "**Description**: Use the fitted Isolation Forest model to predict anomalies.\n",
    "\n",
    "**Steps**:\n",
    "1. Predict anomalies\n",
    "2. Display anomaly counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and prepare the Boston dataset\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "df = boston.frame.dropna()\n",
    "features = df.drop('MEDV', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_forest.fit(scaled_data)\n",
    "\n",
    "# Task 3 - Step 1: Predict anomalies\n",
    "predictions = iso_forest.predict(scaled_data)\n",
    "\n",
    "# Convert predictions to binary format (1 = anomaly, 0 = normal)\n",
    "df['anomaly'] = [1 if p == -1 else 0 for p in predictions]\n",
    "\n",
    "# Task 3 - Step 2: Display anomaly counts\n",
    "anomaly_counts = df['anomaly'].value_counts()\n",
    "print(\"Anomaly Counts:\")\n",
    "print(anomaly_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Visualizing Anomalies\n",
    "**Description**: Visualize the results to see which samples are considered anomalies.\n",
    "\n",
    "**Steps**:\n",
    "1. Plot a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and prepare the Boston dataset\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "df = boston.frame.dropna()\n",
    "features = df.drop('MEDV', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_forest.fit(scaled_data)\n",
    "\n",
    "# Predict anomalies\n",
    "df['anomaly'] = iso_forest.predict(scaled_data)\n",
    "df['anomaly'] = df['anomaly'].map({1: 0, -1: 1})\n",
    "\n",
    "# Step 1: Scatter plot of anomalies (e.g., RM vs MEDV)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='RM', y='MEDV', hue='anomaly', palette={0: 'blue', 1: 'red'})\n",
    "plt.title(\"Anomaly Detection in Boston Housing Data (Red = Anomaly)\")\n",
    "plt.xlabel(\"Average Number of Rooms (RM)\")\n",
    "plt.ylabel(\"Median Value (MEDV)\")\n",
    "plt.legend(title=\"Anomaly\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Interpret Contamination Parameter\n",
    "**Description**: Experiment with different contamination levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and prepare the Boston dataset\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "df = boston.frame.dropna()\n",
    "features = df.drop('MEDV', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "\n",
    "# Try different contamination levels\n",
    "contamination_values = [0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "anomaly_counts = []\n",
    "\n",
    "for c in contamination_values:\n",
    "    model = IsolationForest(contamination=c, random_state=42)\n",
    "    model.fit(scaled_data)\n",
    "    preds = model.predict(scaled_data)\n",
    "    anomalies = [1 if p == -1 else 0 for p in preds]\n",
    "    anomaly_counts.append(sum(anomalies))\n",
    "\n",
    "# Display results\n",
    "results = pd.DataFrame({\n",
    "    'Contamination': contamination_values,\n",
    "    'Anomalies_Detected': anomaly_counts\n",
    "})\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Optional: Visualize the relationship\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results['Contamination'], results['Anomalies_Detected'], marker='o')\n",
    "plt.title('Effect of Contamination Parameter on Anomaly Detection')\n",
    "plt.xlabel('Contamination Level')\n",
    "plt.ylabel('Number of Anomalies Detected')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
