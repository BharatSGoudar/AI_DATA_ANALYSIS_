{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the Titanic dataset:\n",
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "\n",
      "Unique values in 'sex' column:\n",
      "['male' 'female']\n",
      "\n",
      "Value counts for 'sex':\n",
      "sex\n",
      "male      577\n",
      "female    314\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- LABEL ENCODING ---\n",
      "\n",
      "First 5 rows with label encoding:\n",
      "      sex  sex_label_encoded\n",
      "0    male                  1\n",
      "1  female                  0\n",
      "2  female                  0\n",
      "3  female                  0\n",
      "4    male                  1\n",
      "\n",
      "Label Encoding mapping:\n",
      "female -> 0\n",
      "male -> 1\n",
      "\n",
      "--- ONE-HOT ENCODING ---\n",
      "\n",
      "First 5 rows with one-hot encoding (using pandas get_dummies):\n",
      "      sex  sex_female  sex_male\n",
      "0    male       False      True\n",
      "1  female        True     False\n",
      "2  female        True     False\n",
      "3  female        True     False\n",
      "4    male       False      True\n",
      "\n",
      "One-Hot Encoding using scikit-learn:\n",
      "\n",
      "First 5 rows with one-hot encoding (using scikit-learn):\n",
      "      sex  sex_female  sex_male\n",
      "0    male         0.0       1.0\n",
      "1  female         1.0       0.0\n",
      "2  female         1.0       0.0\n",
      "3  female         1.0       0.0\n",
      "4    male         0.0       1.0\n",
      "\n",
      "--- COMPARISON SUMMARY ---\n",
      "Label Encoding:\n",
      "- Transforms categorical values into numerical values\n",
      "- For 'sex' column: female -> 0, male -> 1\n",
      "- Maintains a single column\n",
      "- Introduces ordinal relationship (which may not be appropriate for nominal data)\n",
      "- Memory efficient\n",
      "\n",
      "One-Hot Encoding:\n",
      "- Creates a new binary column for each category\n",
      "- For 'sex' column: creates 'sex_female' and 'sex_male' columns\n",
      "- Expands to multiple columns (one per category)\n",
      "- Avoids ordinal relationship, better for nominal data\n",
      "- Less memory efficient, but more appropriate for machine learning algorithms\n",
      "\n",
      "When to use each:\n",
      "- Label Encoding: Good for ordinal data (e.g., 'low', 'medium', 'high')\n",
      "- One-Hot Encoding: Better for nominal data with no inherent order (e.g., 'sex', 'country')\n",
      "- For binary features like 'sex', both work similarly from a mathematical perspective\n",
      "  but one-hot encoding is generally preferred for consistency and clarity\n",
      "\n",
      "Side-by-side comparison of encodings:\n",
      "  Original  Label Encoded  One-Hot (Female)  One-Hot (Male)\n",
      "0     male              1             False            True\n",
      "1   female              0              True           False\n",
      "2   female              0              True           False\n",
      "3   female              0              True           False\n",
      "4     male              1             False            True\n",
      "5     male              1             False            True\n",
      "6     male              1             False            True\n",
      "7     male              1             False            True\n",
      "8   female              0              True           False\n",
      "9   female              0              True           False\n",
      "\n",
      "--- Question 6: Combining Feature Scaling Techniques ---\n",
      "Original Data:\n",
      "    Feature_A  Feature_B\n",
      "0         10        100\n",
      "1         20         50\n",
      "2         30        200\n",
      "3         40        150\n",
      "4         50        250\n",
      "\n",
      "Min-Max Scaled Data:\n",
      "    Feature_A  Feature_B\n",
      "0       0.00       0.25\n",
      "1       0.25       0.00\n",
      "2       0.50       0.75\n",
      "3       0.75       0.50\n",
      "4       1.00       1.00\n",
      "Explanation: Min-Max Scaling scales data to a fixed range (typically 0 to 1). It's useful when the range of the data is important or for algorithms sensitive to feature magnitude.\n",
      "\n",
      "Standardized Data:\n",
      "    Feature_A  Feature_B\n",
      "0  -1.414214  -0.707107\n",
      "1  -0.707107  -1.414214\n",
      "2   0.000000   0.707107\n",
      "3   0.707107   0.000000\n",
      "4   1.414214   1.414214\n",
      "Explanation: Standardization scales data to have a mean of 0 and a standard deviation of 1. It's less affected by outliers and useful for algorithms that assume data is normally distributed.\n",
      "\n",
      "--- Question 7: Handling Multiple Categorical Features ---\n",
      "Error: titanic.csv not found. Please provide the correct path.\n",
      "Original Categorical Features:\n",
      "       Sex Embarked\n",
      "0    male        S\n",
      "1  female        C\n",
      "2    male        S\n",
      "\n",
      "One-Hot Encoded Features:\n",
      "    Sex_male  Embarked_S\n",
      "0       1.0         1.0\n",
      "1       0.0         0.0\n",
      "2       1.0         1.0\n",
      "\n",
      "--- Question 8: Ordinal Encoding for Ranked Categories ---\n",
      "Pclass column not found in the Titanic dataset (or dataset is empty).\n",
      "\n",
      "--- Question 9: Impact of Scaling on Different Algorithms ---\n",
      "\n",
      "Decision Tree Accuracy (Unscaled): 0.9667\n",
      "Decision Tree Accuracy (Standard Scaled): 0.9667\n",
      "Explanation: Decision Trees make splits based on feature values, so the scale of the features generally doesn't significantly impact their performance.\n",
      "\n",
      "SVM Accuracy (Unscaled): 1.0000\n",
      "SVM Accuracy (Standard Scaled): 1.0000\n",
      "Explanation: SVMs rely on distances between data points. Features with larger values can disproportionately influence the result. Scaling helps to ensure all features contribute more equally.\n",
      "\n",
      "--- Question 10: Custom Transformations for Categorical Features ---\n",
      "Original High Cardinality Data (first 20):\n",
      " 0     A\n",
      "1     B\n",
      "2     C\n",
      "3     A\n",
      "4     A\n",
      "5     D\n",
      "6     E\n",
      "7     A\n",
      "8     B\n",
      "9     F\n",
      "10    G\n",
      "11    A\n",
      "12    B\n",
      "13    H\n",
      "14    I\n",
      "15    A\n",
      "16    B\n",
      "17    J\n",
      "18    K\n",
      "19    A\n",
      "dtype: object\n",
      "\n",
      "Categories with count less than 5: ['B', 'Y', 'X', 'W', 'V', 'U', 'T', 'S', 'R', 'Q', 'P', 'O', 'N', 'M', 'L', 'K', 'J', 'I', 'H', 'G', 'F', 'E', 'D', 'C', 'Z']\n",
      "\n",
      "Custom Encoded High Cardinality Data (first 20):\n",
      " 0         A\n",
      "1     Other\n",
      "2     Other\n",
      "3         A\n",
      "4         A\n",
      "5     Other\n",
      "6     Other\n",
      "7         A\n",
      "8     Other\n",
      "9     Other\n",
      "10    Other\n",
      "11        A\n",
      "12    Other\n",
      "13    Other\n",
      "14    Other\n",
      "15        A\n",
      "16    Other\n",
      "17    Other\n",
      "18    Other\n",
      "19        A\n",
      "dtype: object\n",
      "\n",
      "Value Counts after Custom Encoding:\n",
      " Other    28\n",
      "A        14\n",
      "Name: count, dtype: int64\n",
      "Explanation: This custom transformation groups infrequent categories into a single 'Other' category. This can help reduce the dimensionality introduced by one-hot encoding for high cardinality features and potentially improve model performance by focusing on more frequent patterns.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Question 5: Label Encoding vs One-Hot Encoding\n",
    "# Task: Show the difference between Label Encoding and One-Hot Encoding on the Titanic dataset for the 'Sex' feature.\n",
    "# Label Encoding vs One-Hot Encoding on Titanic Dataset\n",
    "# This script demonstrates the difference between Label Encoding and One-Hot Encoding\n",
    "# using the 'Sex' feature from the Titanic dataset\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Titanic dataset\n",
    "# For this example, we'll use the seaborn library which has the Titanic dataset built-in\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the Titanic dataset:\")\n",
    "print(titanic.head())\n",
    "\n",
    "# Look at the unique values in the 'sex' column\n",
    "print(\"\\nUnique values in 'sex' column:\")\n",
    "print(titanic['sex'].unique())\n",
    "\n",
    "# Check the value counts for 'sex'\n",
    "print(\"\\nValue counts for 'sex':\")\n",
    "print(titanic['sex'].value_counts())\n",
    "\n",
    "# Create a copy of the dataset to work with\n",
    "df = titanic.copy()\n",
    "\n",
    "# LABEL ENCODING\n",
    "print(\"\\n--- LABEL ENCODING ---\")\n",
    "\n",
    "# Create a label encoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'sex' column\n",
    "df['sex_label_encoded'] = label_encoder.fit_transform(df['sex'])\n",
    "\n",
    "# Display the first few rows with label encoding\n",
    "print(\"\\nFirst 5 rows with label encoding:\")\n",
    "print(df[['sex', 'sex_label_encoded']].head())\n",
    "\n",
    "# Show the mapping between original values and encoded values\n",
    "print(\"\\nLabel Encoding mapping:\")\n",
    "for i, category in enumerate(label_encoder.classes_):\n",
    "    print(f\"{category} -> {i}\")\n",
    "\n",
    "# ONE-HOT ENCODING\n",
    "print(\"\\n--- ONE-HOT ENCODING ---\")\n",
    "\n",
    "# Method 1: Using pandas get_dummies\n",
    "df_dummies = pd.get_dummies(df['sex'], prefix='sex')\n",
    "\n",
    "# Add the one-hot encoded columns to the original dataframe\n",
    "df = pd.concat([df, df_dummies], axis=1)\n",
    "\n",
    "# Display the first few rows with one-hot encoding\n",
    "print(\"\\nFirst 5 rows with one-hot encoding (using pandas get_dummies):\")\n",
    "print(df[['sex', 'sex_female', 'sex_male']].head())\n",
    "\n",
    "# Method 2: Using scikit-learn OneHotEncoder\n",
    "print(\"\\nOne-Hot Encoding using scikit-learn:\")\n",
    "# Create a one-hot encoder object\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Reshape the data to fit into the encoder\n",
    "sex_reshaped = df['sex'].values.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the data\n",
    "sex_onehot = onehot_encoder.fit_transform(sex_reshaped)\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded values\n",
    "sex_onehot_df = pd.DataFrame(\n",
    "    sex_onehot, \n",
    "    columns=[f\"sex_{category}\" for category in onehot_encoder.categories_[0]],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Display the first few rows with one-hot encoding\n",
    "print(\"\\nFirst 5 rows with one-hot encoding (using scikit-learn):\")\n",
    "print(pd.concat([df['sex'].reset_index(drop=True), sex_onehot_df.reset_index(drop=True)], axis=1).head())\n",
    "\n",
    "# VISUALIZE THE DIFFERENCES\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Label Encoding\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='sex', hue='sex_label_encoded', data=df)\n",
    "plt.title('Label Encoding of Sex Feature')\n",
    "plt.xlabel('Original Sex Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['female', 'male'])\n",
    "\n",
    "# Plot 2: One-Hot Encoding\n",
    "plt.subplot(1, 2, 2)\n",
    "# Create a temporary dataframe for plotting\n",
    "temp_df = df.melt(id_vars=['sex'], value_vars=['sex_female', 'sex_male'], \n",
    "                  var_name='one_hot_category', value_name='is_category')\n",
    "temp_df = temp_df[temp_df['is_category'] == 1]  # Only keep rows where the category is present\n",
    "sns.countplot(x='sex', hue='one_hot_category', data=temp_df)\n",
    "plt.title('One-Hot Encoding of Sex Feature')\n",
    "plt.xlabel('Original Sex Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['female', 'male'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('encoding_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# COMPARISON SUMMARY\n",
    "print(\"\\n--- COMPARISON SUMMARY ---\")\n",
    "print(\"Label Encoding:\")\n",
    "print(\"- Transforms categorical values into numerical values\")\n",
    "print(\"- For 'sex' column: female -> 0, male -> 1\")\n",
    "print(\"- Maintains a single column\")\n",
    "print(\"- Introduces ordinal relationship (which may not be appropriate for nominal data)\")\n",
    "print(\"- Memory efficient\")\n",
    "\n",
    "print(\"\\nOne-Hot Encoding:\")\n",
    "print(\"- Creates a new binary column for each category\")\n",
    "print(\"- For 'sex' column: creates 'sex_female' and 'sex_male' columns\")\n",
    "print(\"- Expands to multiple columns (one per category)\")\n",
    "print(\"- Avoids ordinal relationship, better for nominal data\")\n",
    "print(\"- Less memory efficient, but more appropriate for machine learning algorithms\")\n",
    "\n",
    "print(\"\\nWhen to use each:\")\n",
    "print(\"- Label Encoding: Good for ordinal data (e.g., 'low', 'medium', 'high')\")\n",
    "print(\"- One-Hot Encoding: Better for nominal data with no inherent order (e.g., 'sex', 'country')\")\n",
    "print(\"- For binary features like 'sex', both work similarly from a mathematical perspective\")\n",
    "print(\"  but one-hot encoding is generally preferred for consistency and clarity\")\n",
    "\n",
    "# Prepare a neat data comparison for final display\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': df['sex'].head(10),\n",
    "    'Label Encoded': df['sex_label_encoded'].head(10),\n",
    "    'One-Hot (Female)': df['sex_female'].head(10),\n",
    "    'One-Hot (Male)': df['sex_male'].head(10)\n",
    "})\n",
    "\n",
    "print(\"\\nSide-by-side comparison of encodings:\")\n",
    "print(comparison_df)\n",
    "\n",
    "\n",
    "\n",
    "# --- Question 6: Combining Feature Scaling Techniques ---\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "print(\"\\n--- Question 6: Combining Feature Scaling Techniques ---\")\n",
    "data_scaling = pd.DataFrame({'Feature_A': [10, 20, 30, 40, 50],\n",
    "                             'Feature_B': [100, 50, 200, 150, 250]})\n",
    "print(\"Original Data:\\n\", data_scaling)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_minmax = scaler_minmax.fit_transform(data_scaling)\n",
    "scaled_minmax_df = pd.DataFrame(scaled_minmax, columns=data_scaling.columns)\n",
    "print(\"\\nMin-Max Scaled Data:\\n\", scaled_minmax_df)\n",
    "print(\"Explanation: Min-Max Scaling scales data to a fixed range (typically 0 to 1). It's useful when the range of the data is important or for algorithms sensitive to feature magnitude.\")\n",
    "\n",
    "# Standardization (Z-score Scaling)\n",
    "scaler_standard = StandardScaler()\n",
    "scaled_standard = scaler_standard.fit_transform(data_scaling)\n",
    "scaled_standard_df = pd.DataFrame(scaled_standard, columns=data_scaling.columns)\n",
    "print(\"\\nStandardized Data:\\n\", scaled_standard_df)\n",
    "print(\"Explanation: Standardization scales data to have a mean of 0 and a standard deviation of 1. It's less affected by outliers and useful for algorithms that assume data is normally distributed.\")\n",
    "\n",
    "# --- Question 7: Handling Multiple Categorical Features ---\n",
    "print(\"\\n--- Question 7: Handling Multiple Categorical Features ---\")\n",
    "# Load the Titanic dataset (replace 'titanic.csv' with the actual path)\n",
    "try:\n",
    "    titanic_data = pd.read_csv('titanic.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: titanic.csv not found. Please provide the correct path.\")\n",
    "    titanic_data = pd.DataFrame({'Sex': ['male', 'female', 'male'], 'Embarked': ['S', 'C', 'S']}) # Sample data\n",
    "\n",
    "if not titanic_data.empty:\n",
    "    categorical_features = ['Sex', 'Embarked']\n",
    "    encoder_onehot = OneHotEncoder(sparse_output=False, drop='first') # drop='first' to avoid multicollinearity\n",
    "    encoded_features = encoder_onehot.fit_transform(titanic_data[categorical_features])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder_onehot.get_feature_names_out(categorical_features))\n",
    "    print(\"Original Categorical Features:\\n\", titanic_data[categorical_features].head())\n",
    "    print(\"\\nOne-Hot Encoded Features:\\n\", encoded_df.head())\n",
    "\n",
    "\n",
    "print(\"\\n--- Question 8: Ordinal Encoding for Ranked Categories ---\")\n",
    "if not titanic_data.empty and 'Pclass' in titanic_data.columns:\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[['3rd', '2nd', '1st']]) # Define the order\n",
    "    titanic_data['Pclass_Encoded'] = ordinal_encoder.fit_transform(titanic_data[['Pclass']])\n",
    "    print(\"Original Pclass:\\n\", titanic_data['Pclass'].head())\n",
    "    print(\"\\nOrdinal Encoded Pclass:\\n\", titanic_data['Pclass_Encoded'].head())\n",
    "else:\n",
    "    print(\"Pclass column not found in the Titanic dataset (or dataset is empty).\")\n",
    "\n",
    "# --- Question 9: Impact of Scaling on Different Algorithms ---\n",
    "print(\"\\n--- Question 9: Impact of Scaling on Different Algorithms ---\")\n",
    "# Create a synthetic dataset\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "dt_unscaled = DecisionTreeClassifier(random_state=42)\n",
    "dt_unscaled.fit(X_train, y_train)\n",
    "y_pred_dt_unscaled = dt_unscaled.predict(X_test)\n",
    "accuracy_dt_unscaled = accuracy_score(y_test, y_pred_dt_unscaled)\n",
    "print(f\"\\nDecision Tree Accuracy (Unscaled): {accuracy_dt_unscaled:.4f}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "dt_scaled = DecisionTreeClassifier(random_state=42)\n",
    "dt_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_dt_scaled = dt_scaled.predict(X_test_scaled)\n",
    "accuracy_dt_scaled = accuracy_score(y_test, y_pred_dt_scaled)\n",
    "print(f\"Decision Tree Accuracy (Standard Scaled): {accuracy_dt_scaled:.4f}\")\n",
    "print(\"Explanation: Decision Trees make splits based on feature values, so the scale of the features generally doesn't significantly impact their performance.\")\n",
    "\n",
    "\n",
    "svm_unscaled = SVC(random_state=42)\n",
    "svm_unscaled.fit(X_train, y_train)\n",
    "y_pred_svm_unscaled = svm_unscaled.predict(X_test)\n",
    "accuracy_svm_unscaled = accuracy_score(y_test, y_pred_svm_unscaled)\n",
    "print(f\"\\nSVM Accuracy (Unscaled): {accuracy_svm_unscaled:.4f}\")\n",
    "\n",
    "svm_scaled = SVC(random_state=42)\n",
    "svm_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_svm_scaled = svm_scaled.predict(X_test_scaled)\n",
    "accuracy_svm_scaled = accuracy_score(y_test, y_pred_svm_scaled)\n",
    "print(f\"SVM Accuracy (Standard Scaled): {accuracy_svm_scaled:.4f}\")\n",
    "print(\"Explanation: SVMs rely on distances between data points. Features with larger values can disproportionately influence the result. Scaling helps to ensure all features contribute more equally.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Question 10: Custom Transformations for Categorical Features ---\")\n",
    "high_cardinality_data = pd.Series(['A', 'B', 'C', 'A', 'A', 'D', 'E', 'A', 'B', 'F', 'G', 'A', 'B', 'H', 'I', 'A', 'B', 'J', 'K', 'A', 'L', 'M', 'A', 'N', 'O', 'A', 'P', 'Q', 'A', 'R', 'S', 'A', 'T', 'U', 'A', 'V', 'W', 'A', 'X', 'Y', 'A', 'Z'])\n",
    "value_counts = high_cardinality_data.value_counts()\n",
    "rare_threshold = 5\n",
    "rare_categories = value_counts[value_counts < rare_threshold].index\n",
    "\n",
    "def custom_encoding(series, rare_threshold=5, rare_name='Other'):\n",
    "    \"\"\"Encodes a categorical series by grouping rare values.\"\"\"\n",
    "    value_counts = series.value_counts()\n",
    "    rare_categories = value_counts[value_counts < rare_threshold].index\n",
    "    return series.apply(lambda x: rare_name if x in rare_categories else x)\n",
    "\n",
    "encoded_high_cardinality = custom_encoding(high_cardinality_data, rare_threshold=rare_threshold)\n",
    "print(\"Original High Cardinality Data (first 20):\\n\", high_cardinality_data.head(20))\n",
    "print(f\"\\nCategories with count less than {rare_threshold}: {list(rare_categories)}\")\n",
    "print(\"\\nCustom Encoded High Cardinality Data (first 20):\\n\", encoded_high_cardinality.head(20))\n",
    "print(f\"\\nValue Counts after Custom Encoding:\\n\", encoded_high_cardinality.value_counts())\n",
    "print(\"Explanation: This custom transformation groups infrequent categories into a single 'Other' category. This can help reduce the dimensionality introduced by one-hot encoding for high cardinality features and potentially improve model performance by focusing on more frequent patterns.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
